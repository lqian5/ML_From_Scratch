{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation\n",
    "Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. \n",
    "To simplify the process, imagine it as recursive binary partitions. We first splits the space first into two regions, the model the response by the mean/majority of Y in each region. Then one of both of these regions are split into two regions, and this process continues until some stopping rule. The preferred stopping rule is when some minimum node size (say 5) is reached.\n",
    "\n",
    "When when we grow the tree, we need to decide the splitting variables and split points based on impurity of the data points on each node. \n",
    "For regression we used the squared-error node impurity measure. For classification, there are different measures:\n",
    "- Misclassification error\n",
    "- Gini index\n",
    "- cross-entropy or deviance\n",
    "while the last two are differential and preferred to be used in splitting variables.\n",
    "\n",
    "Not implemented: Pruning.\n",
    "Cost complexity pruning. At each pruning iteration, a subtree is chosen to be removed and replaced by a leaf node. The choice is based not to hurt prediction accuracy. \n",
    "\n",
    "### Time complexity\n",
    "m is number of features, n is number of observations, d is depth = O(logn) for balanced tree\n",
    "\n",
    "- Training (batch): most efficient is O(mnd). I think my implementation is O(mn^2d)\n",
    "- Inferencing: O(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"\n",
    "    Class that represents a decision node or leaf in the decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.split_feature = feature_i  \n",
    "        self.split_threshold = threshold\n",
    "        self.value = value  # Value if the node is a leaf in the tree\n",
    "        self.left = left    # 'Left' subtree. feature_i <= threshold\n",
    "        self.right = right  # 'Right' subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, tree_type, min_samples_split=5, max_depth=float(\"inf\"), min_impurity=1e-7):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_type = tree_type\n",
    "        self.min_impurity = min_impurity\n",
    "        self.root = None\n",
    "        \n",
    "    def _calculate_entropy(self, labels):\n",
    "        total_count = (len(labels))\n",
    "        class_probabilities = [cnt / total_count for cnt in Counter(labels).values()]\n",
    "        return sum(-p * math.log(p, 2) for p in class_probabilities if p>0)\n",
    "    \n",
    "    def _calculate_impurity(self, R1, R2):\n",
    "        # calculate impurity of two branch, rather than information gain and variance reduction\n",
    "        # which is parent measure minus impurity value calculated below\n",
    "        y1 = R1[:,-1]\n",
    "        y2 = R2[:,-1]\n",
    "        \n",
    "        total_cnt = len(y1) + len(y2)\n",
    "        p1 = len(y1) / total_cnt\n",
    "        p2 = 1- p1\n",
    "        \n",
    "        if self.tree_type == 'regression':\n",
    "            weighted_variance = p1 * np.var(y1) + p2 * np.var(y2)\n",
    "            return weighted_variance\n",
    "        \n",
    "        if self.tree_type == 'classification':\n",
    "            partition_entropy = p1* self._calculate_entropy(y1) + p2 *self._calculate_entropy(y2)\n",
    "            return partition_entropy\n",
    "    \n",
    "        \n",
    "    def _calculate_leaf_value(self, y):\n",
    "        if self.tree_type == 'regression':\n",
    "            # average\n",
    "            return np.mean(y)\n",
    "        if self.tree_type == 'classification':\n",
    "            # majority vote\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \n",
    "        lowest_impurity = float('Inf')\n",
    "        best_split =  None\n",
    "        best_regions = None\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        # find best split\n",
    "        for i in range(num_features):\n",
    "            feature = X[:,i]\n",
    "            unique_values = np.unique(feature)\n",
    "            Xy = np.concatenate((X, y.reshape((len(y), 1))), axis=1)\n",
    "            \n",
    "            for thresh in unique_values:\n",
    "                R1 = Xy[feature <= thresh]\n",
    "                R2 = Xy[feature > thresh]\n",
    "                impurity = self._calculate_impurity(R1, R2)\n",
    "                \n",
    "                if impurity < lowest_impurity:\n",
    "                    lowest_impurity = impurity\n",
    "                    best_split = {'feature_i': i, \"threshold\": thresh}\n",
    "                    best_regions = {'R1': R1, 'R2': R2}\n",
    "                \n",
    "        treeNode = DecisionNode(feature_i=best_split['feature_i'], \n",
    "                                threshold=best_split['threshold'])\n",
    "        \n",
    "        R1, R2 = best_regions['R1'], best_regions['R2']\n",
    "        print(f'Best split is at feature {i}, value {thresh} and the impurity score of this split is {impurity}')\n",
    "        print(f'The best split splits data to two partitions of size {R1.shape[0]} and {R2.shape[0]}')\n",
    "        # a leaf\n",
    "        if depth == self.max_depth or lowest_impurity< self.min_impurity or \\\n",
    "            R1.shape[0] < self.min_samples_split or R2.shape[0] < self.min_samples_split:\n",
    "            treeNode.value = self._calculate_leaf_value(y)\n",
    "            return treeNode\n",
    "           \n",
    "        # a decision node\n",
    "        if R1.shape[0] >= self.min_samples_split:\n",
    "            treeNode.left = self._grow_tree(R1[:,:-1],R1[:,-1], depth+1)\n",
    "        \n",
    "        if  R2.shape[0] >= self.min_samples_split:\n",
    "            treeNode.right = self._grow_tree(R2[:,:-1],R2[:,-1], depth+1)\n",
    "            \n",
    "        return treeNode\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y,0)\n",
    "        \n",
    "        \n",
    "    def predict_value(self, x, node=None):\n",
    "        if not node:\n",
    "            node = self.root\n",
    "        \n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.split_feature] <= node.split_threshold:\n",
    "            return self.predict_value(x, node.left)\n",
    "        else:\n",
    "            return self.predict_value(x, node.right)\n",
    "        \n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        y_pred = [self.predict_value(x) for x in test_X]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on sklearn iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 1.5834545859901241\n",
      "The best split splits data to two partitions of size 35 and 77\n",
      "Best split is at feature 3, value 0.6 and the impurity score of this split is 0.0\n",
      "The best split splits data to two partitions of size 1 and 34\n",
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 0.9998783322990061\n",
      "The best split splits data to two partitions of size 35 and 42\n",
      "Best split is at feature 3, value 1.7 and the impurity score of this split is 0.18717625687320816\n",
      "The best split splits data to two partitions of size 34 and 1\n",
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 0.5266170655714282\n",
      "The best split splits data to two partitions of size 16 and 26\n",
      "Best split is at feature 3, value 2.4 and the impurity score of this split is 0.8960382325345575\n",
      "The best split splits data to two partitions of size 6 and 10\n",
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 0.0\n",
      "The best split splits data to two partitions of size 1 and 25\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTree(\"classification\", min_samples_split=3, max_depth=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the test set is 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = sum(y_test == model.predict(X_test)) / len(y_test)\n",
    "print(f'accuracy on the test set is {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Bagging\n",
    "A complicated decision tree will have a low bias but high variance. Bagging is effective in reducing variance as it averages estimates of models fitted on a large of bootstrap samples. \n",
    "\n",
    "Out-of-Bag Error Estimation. On averate, each bagged tree makes use of around two-thirds of the observations (The chance of one point not selected in any n draw from n samples with replacement is (1-1/n)^n = 1/e is about 1/3 when n goes to infinity). The remaining one-third of the observations not used is OOB. For each OOB sample, average (regression) all predictions. The resulting OOB error is a valid estimate of the test error for the bagged model.\n",
    "\n",
    "Feature importance is measured by mean drop is RSS or Gini index / entropy over all trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random Forest improves over bagged trees by way of a small tweak that decorrelates the trees. That's, at each split, only a random sample m=sqrt(p) predictors is chosen for consideration, so each tree will not be similar/correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "Trees are built sequentially. In each step, we fit a tree using the current residuals, rather than the outcome Y, and add the new decision tree into the fitted function in order to update the residuals. We slowly improve f in areas where it does not perform well. Smaller trees are typically sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
