{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation\n",
    "Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. \n",
    "To simplify the process, imagine it as recursive binary partitions. We first splits the space first into two regions, the model the response by the mean/majority of Y in each region. Then one of both of these regions are split into two regions, and this process continues until some stopping rule. The preferred stopping rule is when some minimum node size (say 5) is reached.\n",
    "\n",
    "When when we grow the tree, we need to decide the splitting variables and split points based on impurity of the data points on each node. \n",
    "For regression we used the squared-error node impurity measure. For classification, there are different measures:\n",
    "- Misclassification error\n",
    "- Gini index\n",
    "- cross-entropy or deviance\n",
    "while the last two are differential and preferred to be used in splitting variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"\n",
    "    Class that represents a decision node or leaf in the decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.split_feature = feature_i  \n",
    "        self.split_threshold = threshold\n",
    "        self.value = value  # Value if the node is a leaf in the tree\n",
    "        self.left = left    # 'Left' subtree. feature_i <= threshold\n",
    "        self.right = right  # 'Right' subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, tree_type, min_samples_split=5, max_depth=float(\"inf\"), min_impurity=1e-7):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_type = tree_type\n",
    "        self.min_impurity = min_impurity\n",
    "        self.root = None\n",
    "        \n",
    "    def _calculate_entropy(self, labels):\n",
    "        total_count = (len(labels))\n",
    "        class_probabilities = [cnt / total_count for cnt in Counter(labels).values()]\n",
    "        return sum(-p * math.log(p, 2) for p in class_probabilities if p>0)\n",
    "    \n",
    "    def _calculate_impurity(self, R1, R2):\n",
    "        # calculate impurity of two branch, rather than information gain and variance reduction\n",
    "        # which is parent measure minus impurity value calculated below\n",
    "        y1 = R1[:,-1]\n",
    "        y2 = R2[:,-1]\n",
    "        \n",
    "        total_cnt = len(y1) + len(y2)\n",
    "        p1 = len(y1) / total_cnt\n",
    "        p2 = 1- p1\n",
    "        \n",
    "        if self.tree_type == 'regression':\n",
    "            weighted_variance = p1 * np.var(y1) + p2 * np.var(y2)\n",
    "            return weighted_variance\n",
    "        \n",
    "        if self.tree_type == 'classification':\n",
    "            partition_entropy = p1* self._calculate_entropy(y1) + p2 *self._calculate_entropy(y2)\n",
    "            return partition_entropy\n",
    "    \n",
    "        \n",
    "    def _calculate_leaf_value(self, y):\n",
    "        if self.tree_type == 'regression':\n",
    "            # average\n",
    "            return np.mean(y)\n",
    "        if self.tree_type == 'classification':\n",
    "            # majority vote\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \n",
    "        lowest_impurity = float('Inf')\n",
    "        best_split =  None\n",
    "        best_regions = None\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        # find best split\n",
    "        for i in range(num_features):\n",
    "            feature = X[:,i]\n",
    "            unique_values = np.unique(feature)\n",
    "            Xy = np.concatenate((X, y.reshape((len(y), 1))), axis=1)\n",
    "            \n",
    "            for thresh in unique_values:\n",
    "                R1 = Xy[feature <= thresh]\n",
    "                R2 = Xy[feature > thresh]\n",
    "                impurity = self._calculate_impurity(R1, R2)\n",
    "                \n",
    "                if impurity < lowest_impurity:\n",
    "                    lowest_impurity = impurity\n",
    "                    best_split = {'feature_i': i, \"threshold\": thresh}\n",
    "                    best_regions = {'R1': R1, 'R2': R2}\n",
    "                \n",
    "        treeNode = DecisionNode(feature_i=best_split['feature_i'], \n",
    "                                threshold=best_split['threshold'])\n",
    "        \n",
    "        R1, R2 = best_regions['R1'], best_regions['R2']\n",
    "        print(f'Best split is at feature {i}, value {thresh} and the impurity score of this split is {impurity}')\n",
    "        print(f'The best split splits data to two partitions of size {R1.shape[0]} and {R2.shape[0]}')\n",
    "        # a leaf\n",
    "        if depth == self.max_depth or lowest_impurity< self.min_impurity or \\\n",
    "            R1.shape[0] < self.min_samples_split or R2.shape[0] < self.min_samples_split:\n",
    "            treeNode.value = self._calculate_leaf_value(y)\n",
    "            return treeNode\n",
    "           \n",
    "        # a decision node\n",
    "        if R1.shape[0] >= self.min_samples_split:\n",
    "            treeNode.left = self._grow_tree(R1[:,:-1],R1[:,-1], depth+1)\n",
    "        \n",
    "        if  R2.shape[0] >= self.min_samples_split:\n",
    "            treeNode.right = self._grow_tree(R2[:,:-1],R2[:,-1], depth+1)\n",
    "            \n",
    "        return treeNode\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y,0)\n",
    "        \n",
    "        \n",
    "    def predict_value(self, x, node=None):\n",
    "        if not node:\n",
    "            node = self.root\n",
    "        \n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.split_feature] <= node.split_threshold:\n",
    "            return self.predict_value(x, node.left)\n",
    "        else:\n",
    "            return self.predict_value(x, node.right)\n",
    "        \n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        y_pred = [self.predict_value(x) for x in test_X]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on sklearn iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 1.5834545859901241\n",
      "The best split splits data to two partitions of size 35 and 77\n",
      "Best split is at feature 3, value 0.6 and the impurity score of this split is 0.0\n",
      "The best split splits data to two partitions of size 1 and 34\n",
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 0.9998783322990061\n",
      "The best split splits data to two partitions of size 35 and 42\n",
      "Best split is at feature 3, value 1.7 and the impurity score of this split is 0.18717625687320816\n",
      "The best split splits data to two partitions of size 34 and 1\n",
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 0.5266170655714282\n",
      "The best split splits data to two partitions of size 16 and 26\n",
      "Best split is at feature 3, value 2.4 and the impurity score of this split is 0.8960382325345575\n",
      "The best split splits data to two partitions of size 6 and 10\n",
      "Best split is at feature 3, value 2.5 and the impurity score of this split is 0.0\n",
      "The best split splits data to two partitions of size 1 and 25\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTree(\"classification\", min_samples_split=3, max_depth=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the test set is 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = sum(y_test == model.predict(X_test)) / len(y_test)\n",
    "print(f'accuracy on the test set is {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
