{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Implementation\n",
    "Logistic regression is a classification model; classification is the task of choosing a value of y that maximizes P(Y|X). Logistic regression makes a central assumption that P(Y|X) can be approximated as a sigmoid function applied to a linear combination of input features.\n",
    "\n",
    "There are two main methods that approximate logistic regression parameters: Newton-Raphson method and gradient descent. Newton-Raphson is a traditional numerical approximation method in mathematics. Due to its drawback like computation time on calculating Hessian matrix and its problem on saddle point, gradient descent is more favored and will be used in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Xavier initialisation between [-1/sqrt(N), 1/sqrt(N)]. 1/sqrt(N) is sample variance.\n",
    "        mitigate the problem of disappearing gradients\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.W = np.random.uniform(-limit, limit, (n_features,))\n",
    "        \n",
    "        \n",
    "    def _compute_loss(self, y, y_pred):\n",
    "        '''\n",
    "        Computes log loss (negative log likehood) for binary classification. \n",
    "        In case of multi classification, we need to compute cross entropy\n",
    "        '''\n",
    "        return - y.dot(np.log(y_pred)) - (1-y).dot(np.log(1-y_pred))\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, iterations=10):\n",
    "        self._initialize_parameters(X)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            y_pred = self._sigmoid(X.dot(self.W))\n",
    "            grad_w = (y - y_pred).dot(X)\n",
    "            self.W -= self.learning_rate * (-grad_w)\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            print(f'Epoch {i}, loss {loss}')\n",
    "        print(f'Coefficients: {self.W}')\n",
    "        \n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        return np.round(self._sigmoid(test_X.dot(self.W))).astype(int)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [4, 5], [3, 5]])\n",
    "y = np.array([1, 0, 0])\n",
    "\n",
    "\n",
    "test_X = np.array([[1, 1], [2, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 3.6152994717265132\n",
      "Epoch 1, loss 1.7614170574478591\n",
      "Epoch 2, loss 1.5314894543498545\n",
      "Epoch 3, loss 1.430105404773753\n",
      "Epoch 4, loss 1.4085096414386844\n",
      "Epoch 5, loss 1.402923073462688\n",
      "Epoch 6, loss 1.3981137588804897\n",
      "Epoch 7, loss 1.3933680828506447\n",
      "Epoch 8, loss 1.3886765064055167\n",
      "Epoch 9, loss 1.3840382024323081\n",
      "Coefficients: [-0.16622661 -0.27761948]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y) # loss should be positive?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
